distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:  tcp://10.128.5.206:44019
distributed.scheduler - INFO -   dashboard at:                     :8787
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:45741
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:45741
distributed.worker - INFO -          dashboard at:         10.128.50.79:35841
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:40967
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:40967
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xu9kie1r
distributed.worker - INFO -          dashboard at:         10.128.50.79:39457
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ip494dp1
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:45741', name: 160, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:45741
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:40967', name: 164, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:40967
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:39643
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:39643
distributed.worker - INFO -          dashboard at:         10.128.50.78:44511
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ji8q9ld2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:42879
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:42879
distributed.worker - INFO -          dashboard at:        10.128.49.252:37879
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:46525
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ykcjk9s5
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:39643', name: 151, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:39643
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:42879', name: 72, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:42879
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:46525', name: 134, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:46525
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:37705
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:37705
distributed.worker - INFO -          dashboard at:        10.128.49.251:42195
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:34287
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8yltorfc
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:34287
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.128.49.251:46769
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kqobyyyn
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:37705', name: 50, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:37705
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:34287', name: 52, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:34287
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:36389
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:36389
distributed.worker - INFO -          dashboard at:         10.128.5.206:43589
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8oop_xh4
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:36389', name: 9, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:36389
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:44393
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:44393
distributed.worker - INFO -          dashboard at:        10.128.49.251:38979
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qowmspqt
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:44393', name: 49, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:44393
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:40019
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:40019
distributed.worker - INFO -          dashboard at:         10.128.50.79:41075
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3_pbhrvu
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:40019', name: 171, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:40019
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:46525
distributed.worker - INFO -          dashboard at:         10.128.50.48:33265
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-umuvxpji
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:34251
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:34251
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:38677
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:38677
distributed.worker - INFO -          dashboard at:         10.128.50.48:37513
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vxm3gzcb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:41187
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:41187
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:35707
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:35707
distributed.worker - INFO -          dashboard at:          10.128.50.4:35045
distributed.worker - INFO -          dashboard at:          10.128.50.4:36769
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-egz35sre
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b1fq61g_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:34439', name: 86, memory: 0, processing: 0>
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:43245
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:43245
distributed.worker - INFO -          dashboard at:         10.128.50.78:46319
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aathqhfs
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:34439
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:34251', name: 88, memory: 0, processing: 0>
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:45387
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:45387
distributed.worker - INFO -          dashboard at:        10.128.49.252:38531
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hy0679ls
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:34251
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:38677', name: 138, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:38677
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:43245', name: 159, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:43245
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:35707', name: 97, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:35707
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:41187', name: 109, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:41187
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:45387', name: 75, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:45387
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:39633
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:39633
distributed.worker - INFO -          dashboard at:         10.128.8.191:42477
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3g6jujri
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:39633', name: 19, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:39633
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:46779
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:46779
distributed.worker - INFO -          dashboard at:         10.128.8.191:35497
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-skwfsnz0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:34761
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:33659
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:34761
distributed.worker - INFO -          dashboard at:         10.128.8.192:45097
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:33659
distributed.worker - INFO -          dashboard at:         10.128.8.192:40985
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:46779', name: 28, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mmq9emed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b9ag8077
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:46779
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:33659', name: 39, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:33659
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:34761', name: 38, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:34761
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:35515
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:35515
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          dashboard at:          10.128.50.3:46509
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:34439
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:34439
distributed.worker - INFO -          dashboard at:          10.128.50.3:46171
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9ebfyy4u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-va930bc0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:35515', name: 116, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:35515
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:41103', name: 123, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:41103
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:39323
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:39323
distributed.worker - INFO -          dashboard at:         10.128.50.79:36429
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3t614bv0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:39141
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:33247
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:39141
distributed.worker - INFO -          dashboard at:         10.128.5.206:35699
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:33247
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          dashboard at:         10.128.5.206:45123
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bg8qo9uv
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4ez5if0x
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:39323', name: 169, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:39323
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:33247', name: 7, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:33247
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:39141', name: 8, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:39141
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:42043
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:42043
distributed.worker - INFO -          dashboard at:        10.128.49.252:34525
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yi_m75fc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:35047
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:35047
distributed.worker - INFO -          dashboard at:          10.128.50.4:45987
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gox9y_na
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:42043', name: 79, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:42043
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:35047', name: 108, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:35047
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:45007
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:45007
distributed.worker - INFO -          dashboard at:         10.128.50.78:36371
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-krp3vxmy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:36397
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:36397
distributed.worker - INFO -          dashboard at:        10.128.49.251:39215
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lngxawbx
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:45007', name: 155, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:45007
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          dashboard at:         10.128.50.47:46451
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5p_52uem
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:41103
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:41103
distributed.worker - INFO -          dashboard at:         10.128.50.47:38689
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yxu2alcj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:38959
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:38959
distributed.worker - INFO -          dashboard at:         10.128.50.48:36377
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k40c3s0_
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:36397', name: 63, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:36397
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:38959', name: 130, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:38959
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:36273
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:36273
distributed.worker - INFO -          dashboard at:         10.128.8.192:46721
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m8nc60h6
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:36273', name: 40, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:36273
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:42547
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:42547
distributed.worker - INFO -          dashboard at:          10.128.50.3:39465
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w7dkgp6l
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:34585
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:34585
distributed.worker - INFO -          dashboard at:         10.128.5.206:38059
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3cpfysop
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:38821
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:38821
distributed.worker - INFO -          dashboard at:         10.128.5.206:33275
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:37103
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:39605
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:37103
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:39605
distributed.worker - INFO -          dashboard at:         10.128.5.206:39569
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:43327
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-21f2ocz7
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          dashboard at:         10.128.5.206:43525
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:43327
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.5.206:45759
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:41135
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7oq6174s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:41135
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_fhhy1_k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.5.206:39019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2oeonhkn
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fpmc5smd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:36163
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:36163
distributed.worker - INFO -          dashboard at:         10.128.5.206:40819
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:34079
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:38423
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:34079
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:38423
distributed.worker - INFO -          dashboard at:         10.128.5.206:33881
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         10.128.5.206:41731
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ftaqdujg
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3a9vxbf4
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1er2t6sq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:40067
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:40067
distributed.worker - INFO -          dashboard at:         10.128.5.206:37617
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t08cu8pr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:33687
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:33687
distributed.worker - INFO -          dashboard at:         10.128.5.206:42513
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w8o8cwtm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:37117
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:37117
distributed.worker - INFO -          dashboard at:         10.128.8.191:42955
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yi99s_ja
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:40607
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:42547', name: 93, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:40607
distributed.worker - INFO -          dashboard at:         10.128.8.191:40377
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-31_7kme4
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:42547
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:34585', name: 12, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:34585
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:38821', name: 3, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:38821
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:37103', name: 1, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:37103
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:43327', name: 2, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:43327
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:39605', name: 13, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:39605
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:41135', name: 10, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:41135
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:36163', name: 6, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:36163
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:33525
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:33525
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          dashboard at:         10.128.50.79:33647
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:38423', name: 11, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z31x20zt
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:38423
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:34079', name: 4, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:39003
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:39003
distributed.worker - INFO -          dashboard at:         10.128.50.47:41881
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:34079
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j7sf1flz
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.5.206:46657
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:40067', name: 14, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://10.128.5.206:46657
distributed.worker - INFO -          dashboard at:         10.128.5.206:39143
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3sr7yyyr
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:40067
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:33687', name: 15, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:33687
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:37117', name: 29, memory: 0, processing: 0>
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:33417
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:33417
distributed.worker - INFO -          dashboard at:        10.128.49.251:41079
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:37117
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:39435
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-inahr_lq
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:42165
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:42165
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:39435
distributed.worker - INFO -          dashboard at:        10.128.49.251:39677
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:40607', name: 31, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.128.49.251:42031
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d_hmt_k8
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q3b464rk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:33507
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:33507
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:41647
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:46267
distributed.worker - INFO -          dashboard at:        10.128.49.251:44263
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:41647
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:46267
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.128.49.251:42725
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:36645
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:38113
distributed.worker - INFO -          dashboard at:        10.128.49.251:33461
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:38113
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mi_12jer
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:40607
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:36645
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          dashboard at:        10.128.49.251:43263
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:36797
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pck0ud0o
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3oyynzrs
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          dashboard at:        10.128.49.251:39883
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:36797
distributed.worker - INFO -          dashboard at:        10.128.49.251:33213
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:40073
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6c3ep7t3
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:40073
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.128.49.251:34951
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3ikix0mk
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3xp1p7u9
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k4t_iwk7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:44479
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:44479
distributed.worker - INFO -          dashboard at:        10.128.49.251:35783
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hh8_fike
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:37183
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:37183
distributed.worker - INFO -          dashboard at:         10.128.8.192:34883
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-94d27mva
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:37611
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:37611
distributed.worker - INFO -          dashboard at:         10.128.50.79:43881
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:33179
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:41787
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:33179
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:36049
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:41787
distributed.worker - INFO -          dashboard at:         10.128.50.79:34457
distributed.worker - INFO -          dashboard at:         10.128.50.79:36037
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:36049
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          dashboard at:         10.128.50.79:44337
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-38eqq_sq
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-465a06mh
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:41697
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y_6bh6ew
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:41697
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3lvgzbyi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:35809
distributed.worker - INFO -          dashboard at:         10.128.50.79:37803
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:40731
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:35809
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.50.79:40259
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:40731
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          dashboard at:         10.128.50.79:37011
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ztc3962_
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rp5416kx
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nf61cjiv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:36857
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:36857
distributed.worker - INFO -          dashboard at:         10.128.50.79:37247
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2_4t3ewd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:46565
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:46565
distributed.worker - INFO -          dashboard at:         10.128.50.79:41735
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dhsjbig8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:33341
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:33341
distributed.worker - INFO -          dashboard at:         10.128.50.79:41011
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bhjuasea
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:33231
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:33231
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:45151
distributed.worker - INFO -          dashboard at:          10.128.50.4:41063
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:45151
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          dashboard at:          10.128.50.4:36139
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z8rb4ge7
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h3fqubsn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:45811
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:45811
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:35721
distributed.worker - INFO -          dashboard at:          10.128.50.4:42857
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:39383
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:35721
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:39383
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.128.50.4:34867
distributed.worker - INFO -          dashboard at:          10.128.50.4:38051
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:42285
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:37815
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cq9x6xdt
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:42285
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:33525', name: 167, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:37815
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.128.50.4:38159
distributed.worker - INFO -          dashboard at:          10.128.50.4:42881
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xl36cohg
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-15g2baor
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:38853
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:38853
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:44497
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ms84s8t_
distributed.worker - INFO -          dashboard at:          10.128.50.4:38099
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z3qx63ly
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:44497
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.128.50.4:39307
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-21s0k__q
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:34035
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0771p6v8
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:34035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.128.50.4:35091
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fn7jj7ri
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:33525
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:35959
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:35959
distributed.worker - INFO -          dashboard at:          10.128.50.4:38695
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:39003', name: 119, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bzvtpn7f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:42213
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:39003
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:42213
distributed.worker - INFO -          dashboard at:          10.128.50.4:33953
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.5.206:46657', name: 5, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ssltv59q
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.5.206:46657
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:33391
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:37521
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:37521
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:33391
distributed.worker - INFO -          dashboard at:          10.128.50.3:33123
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:42651
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:42017
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:42017
distributed.worker - INFO -          dashboard at:          10.128.50.3:45217
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:37559
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:37559
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:36199
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:42651
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.128.50.3:38457
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.128.50.3:43407
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:36199
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:35913
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:35913
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:39863
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.128.50.3:45389
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-twplbqro
distributed.worker - INFO -          dashboard at:          10.128.50.3:34753
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z32rerj5
distributed.worker - INFO -          dashboard at:          10.128.50.3:36297
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6e2lfglr
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:39863
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.128.50.3:46185
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-slog0bzp
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:41371
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-duwk3m4k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:41371
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lymqulov
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.128.50.3:44467
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:33775
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-li1x34d5
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:33775
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          10.128.50.3:38715
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:36601
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sb3clrr3
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:38431
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i_l8v7fx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:36601
distributed.worker - INFO -          dashboard at:          10.128.50.3:35081
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:38431
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          dashboard at:          10.128.50.3:39865
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jkxkly8n
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tcieaipu
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lwpcvoz_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:34349
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:34349
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:34457
distributed.worker - INFO -          dashboard at:         10.128.8.192:39983
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:32771
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:34457
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:32771
distributed.worker - INFO -          dashboard at:         10.128.8.192:38879
distributed.worker - INFO -          dashboard at:         10.128.8.192:35077
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zmt5y63q
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:33967
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k7yohm_u
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sk6we4c5
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:33967
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.8.192:46017
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_4pl6mpx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:39585
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:39585
distributed.worker - INFO -          dashboard at:         10.128.8.192:35655
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:43859
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:43859
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:41051
distributed.worker - INFO -          dashboard at:         10.128.8.192:35965
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:41051
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.8.192:36183
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h6hlqupd
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p_9r21qf
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:41035
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n_shc451
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:38245
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:41035
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:33417', name: 61, memory: 0, processing: 0>
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:35407
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:38245
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.8.192:33767
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:35407
distributed.worker - INFO -          dashboard at:         10.128.8.192:45575
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.8.192:39907
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-coag6enl
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_3n30ioe
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kssxpx9_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.4:39335
distributed.worker - INFO -          Listening to:    tcp://10.128.50.4:39335
distributed.worker - INFO -          dashboard at:          10.128.50.4:35273
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:34769
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:34769
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         10.128.8.192:46413
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3pro15_u
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-to47tvnd
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:33417
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:42165', name: 62, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:42165
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:39435', name: 48, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:43777
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:43777
distributed.worker - INFO -          dashboard at:         10.128.50.48:40399
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:39435
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p6t54nwa
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:33507', name: 54, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:33507
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:41647', name: 56, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:41647
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:33495
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:33495
distributed.worker - INFO -          dashboard at:         10.128.50.78:45565
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y009umss
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:46267', name: 57, memory: 0, processing: 0>
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:42015
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:42015
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:39691
distributed.worker - INFO -          dashboard at:         10.128.50.78:34319
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:39691
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.50.78:33103
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ob0ki4c1
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:40601
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:40601
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qdz_t6o5
distributed.worker - INFO -          dashboard at:         10.128.50.78:45411
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:36385
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mqj2rbts
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:39275
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:36385
distributed.worker - INFO -          dashboard at:        10.128.49.252:36007
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:39275
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.128.49.252:45419
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1oj0phbq
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:40431
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:33587
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:40049
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:33165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:34807
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:40049
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:33165
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:40195
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:40431
distributed.worker - INFO -          dashboard at:         10.128.50.78:37145
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:33587
distributed.worker - INFO -          dashboard at:         10.128.50.78:37779
distributed.worker - INFO -          dashboard at:        10.128.49.252:46219
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:34807
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:40195
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.128.49.252:40123
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:43247
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.128.49.252:34275
distributed.worker - INFO -          dashboard at:         10.128.50.78:39653
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:40571
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:46267
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iq9z_e1x
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-egfw21i3
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:44837
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:43953
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-14su0wet
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:43247
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-om22azvk
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:40571
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.50.78:42429
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:42191
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:44837
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:43953
distributed.worker - INFO -          dashboard at:         10.128.50.78:37699
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:38113', name: 51, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:44385
distributed.worker - INFO -          dashboard at:        10.128.49.252:38475
distributed.worker - INFO -          dashboard at:        10.128.49.252:40961
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-clbv_eyz
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:42191
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:44385
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        10.128.49.252:34385
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m_qeq9tj
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        10.128.49.252:36871
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-psty3351
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:44035
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:35899
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:35899
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1c9jigfg
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:44035
distributed.worker - INFO -          dashboard at:         10.128.50.78:38243
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9vfucnf4
distributed.worker - INFO -          dashboard at:         10.128.50.47:38999
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8wgc_359
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ocxgyb97
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3lfa89zr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hferikif
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ufy7pqer
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:37365
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5lcpn2bw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:37365
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:42841
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.50.47:32827
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:45477
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:42841
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:45477
distributed.worker - INFO -          dashboard at:        10.128.49.252:41631
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m01ffcau
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.128.49.252:37047
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:38113
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:34001
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:45689
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1bvsoczs
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:34001
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:34351
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:45689
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mzuwrb7e
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:34351
distributed.worker - INFO -          dashboard at:         10.128.50.47:40885
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:36605
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.50.47:33393
distributed.worker - INFO -          dashboard at:        10.128.49.252:36221
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:36797', name: 53, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:36605
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c2mzr7hv
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:33859
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:39269
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:33859
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:39269
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         10.128.50.78:36851
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         10.128.50.78:39183
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vs8auay0
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          dashboard at:         10.128.50.47:33745
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-489d0wbv
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:36655
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:36655
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qfsi2n3d
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         10.128.50.47:33751
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ywtwmrdd
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:36447
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wu9xqxto
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:39241
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:39241
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:36447
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fm94pmqt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.50.47:41873
distributed.worker - INFO -          dashboard at:         10.128.50.47:46209
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:44307
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:44307
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t6kj394l
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ixt1zvw2
distributed.worker - INFO -          dashboard at:         10.128.50.47:34377
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:37573
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:42235
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:37573
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:42235
distributed.worker - INFO -          dashboard at:         10.128.50.47:42253
distributed.worker - INFO -          dashboard at:         10.128.50.47:33845
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-odt15n6e
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:36797
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xvu1_5z3
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vaxe8pf_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:45735
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:45735
distributed.worker - INFO -          dashboard at:         10.128.50.47:32781
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:40073', name: 60, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h04y9xqd
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:40073
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:36645', name: 58, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:36645
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:37239
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:37239
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:44389
distributed.worker - INFO -          dashboard at:         10.128.8.191:39795
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:40193
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:44389
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:40193
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:33115
distributed.worker - INFO -          dashboard at:         10.128.8.191:40463
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:39707
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.251:40665
distributed.worker - INFO -          dashboard at:         10.128.8.191:40565
distributed.worker - INFO -          Listening to:  tcp://10.128.49.251:40665
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        10.128.49.251:39409
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:33115
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:39707
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5o5l4enh
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:41379
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.8.191:44099
distributed.worker - INFO -          dashboard at:         10.128.8.191:42901
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:41379
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p7w6lx_m
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zxavarws
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.8.191:33651
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a8e_tg7d
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:40581
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x31f8uj7
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:42899
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:40581
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t_oka_de
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:42899
distributed.worker - INFO -          dashboard at:         10.128.8.191:40639
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3n9k_1_x
distributed.worker - INFO -          dashboard at:         10.128.8.191:37397
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:37821
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xkvjekv1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_kjn9nd9
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:37821
distributed.worker - INFO -          dashboard at:         10.128.8.191:41377
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mgmqb_zw
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:46345
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:46345
distributed.worker - INFO -          dashboard at:         10.128.8.191:39625
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g22u_4na
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:41257
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:41257
distributed.worker - INFO -          dashboard at:         10.128.8.191:44795
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gevzi3nk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.79:36983
distributed.worker - INFO -          Listening to:   tcp://10.128.50.79:36983
distributed.worker - INFO -          dashboard at:         10.128.50.79:46445
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0eehwn2o
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:32777
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:32777
distributed.worker - INFO -          dashboard at:         10.128.50.48:35443
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_mn70r3m
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:43851
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:43851
distributed.worker - INFO -          dashboard at:         10.128.50.48:43783
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xfkfuxxj
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:40137
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:44923
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:40137
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:44923
distributed.worker - INFO -          dashboard at:         10.128.50.48:34575
distributed.worker - INFO -          dashboard at:         10.128.50.48:39319
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:36715
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:36715
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         10.128.50.48:40189
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rtlaqt9c
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3z1fanca
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x0tg2sk4
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:34107
distributed.worker - INFO -       Start worker at:  tcp://10.128.49.252:45421
distributed.worker - INFO -       Start worker at:    tcp://10.128.50.3:40381
distributed.worker - INFO -          Listening to:    tcp://10.128.50.3:40381
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:41287
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.128.49.252:45421
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:34107
distributed.worker - INFO -          dashboard at:        10.128.49.252:39575
distributed.worker - INFO -          dashboard at:          10.128.50.3:39013
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:41287
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          dashboard at:         10.128.50.48:44883
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.50.48:34889
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6_06uv57
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tf048g1h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-teka2931
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.78:42245
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v2ufhv7q
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.50.78:42245
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:35215
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:46263
distributed.worker - INFO -          dashboard at:         10.128.50.78:35727
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:35215
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:46263
distributed.worker - INFO -          dashboard at:         10.128.50.48:43381
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO -          dashboard at:         10.128.50.48:44365
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gff71eyd
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ddchmhbb
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6pf0elre
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:38729
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:38729
distributed.worker - INFO -          dashboard at:         10.128.50.48:34739
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hcvtv0u2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:35699
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:35699
distributed.worker - INFO -          dashboard at:         10.128.50.48:34223
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9enu57fg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.192:43473
distributed.worker - INFO -          Listening to:   tcp://10.128.8.192:43473
distributed.worker - INFO -          dashboard at:         10.128.8.192:36869
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kg_jkbgk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.8.191:35419
distributed.worker - INFO -          Listening to:   tcp://10.128.8.191:35419
distributed.worker - INFO -          dashboard at:         10.128.8.191:40605
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oj6e6rlh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.48:43629
distributed.worker - INFO -          Listening to:   tcp://10.128.50.48:43629
distributed.worker - INFO -          dashboard at:         10.128.50.48:44575
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e6scvp10
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:44479', name: 55, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:44479
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:37183', name: 47, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:37183
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:33179', name: 174, memory: 0, processing: 0>
distributed.worker - INFO -       Start worker at:   tcp://10.128.50.47:33037
distributed.worker - INFO -          Listening to:   tcp://10.128.50.47:33037
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.128.50.47:33435
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dqssrls8
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:33179
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:36049', name: 163, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:36049
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:41787', name: 168, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:41787
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:37611', name: 172, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:37611
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:41697', name: 161, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:41697
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:35809', name: 175, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:35809
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:40731', name: 166, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:40731
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:36857', name: 162, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:36857
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:46565', name: 165, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:46565
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:33341', name: 173, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:33341
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:45151', name: 100, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:45151
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:33231', name: 104, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:33231
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:39383', name: 106, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:39383
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:35721', name: 102, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:35721
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:45811', name: 107, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:45811
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:37815', name: 105, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:37815
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:42285', name: 111, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:42285
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:38853', name: 98, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:38853
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:44497', name: 99, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:44497
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:34035', name: 101, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:34035
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:35959', name: 110, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:35959
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:42213', name: 96, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:42213
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:42017', name: 84, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:42017
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:33391', name: 82, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:33391
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:37521', name: 91, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:37521
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:36199', name: 95, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:36199
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:37559', name: 94, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:37559
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:39863', name: 92, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:39863
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:41371', name: 80, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:41371
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:36601', name: 85, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:36601
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:38431', name: 87, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:38431
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:35913', name: 90, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:35913
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:33775', name: 81, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:33775
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:42651', name: 83, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:42651
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:34457', name: 46, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:34457
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:34349', name: 35, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:34349
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:32771', name: 36, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:32771
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:33967', name: 42, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:33967
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:43859', name: 44, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:43859
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:41051', name: 41, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:41051
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:39585', name: 45, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:39585
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:41035', name: 34, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:41035
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:38245', name: 33, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:38245
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:35407', name: 43, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:35407
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:39335', name: 103, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:39335
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:34769', name: 32, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:34769
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:43777', name: 136, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:43777
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:33495', name: 144, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:33495
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:42015', name: 146, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:42015
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:39691', name: 158, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:39691
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:40601', name: 157, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:40601
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:36385', name: 71, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:36385
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:33165', name: 153, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:33165
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:40431', name: 73, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:40431
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:40049', name: 147, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:40049
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:40195', name: 150, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:40195
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:39275', name: 67, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:39275
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:33587', name: 74, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:33587
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:34807', name: 77, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:34807
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:40571', name: 149, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:40571
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:42191', name: 66, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:42191
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:44385', name: 70, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:44385
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:44837', name: 68, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:44837
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:43953', name: 76, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:43953
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:43247', name: 154, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:43247
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:42841', name: 65, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:42841
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:45477', name: 64, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:45477
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:35899', name: 156, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:35899
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:44035', name: 125, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:44035
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:34351', name: 78, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:34351
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:37365', name: 113, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:37365
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:45689', name: 120, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:45689
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:34001', name: 114, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:34001
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:33859', name: 115, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:33859
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:39269', name: 152, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:39269
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:36605', name: 145, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:36605
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:36655', name: 124, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:36655
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:36447', name: 126, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:36447
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:39241', name: 127, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:39241
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:44307', name: 117, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:44307
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:42235', name: 121, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:42235
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:37573', name: 122, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:37573
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:45735', name: 118, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:45735
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:37239', name: 17, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:37239
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:44389', name: 23, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:44389
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:40193', name: 18, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:40193
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:33115', name: 20, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:33115
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:39707', name: 27, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:39707
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:41379', name: 21, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:41379
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:42899', name: 22, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:42899
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.251:40665', name: 59, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.251:40665
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:40581', name: 25, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:40581
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:37821', name: 26, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:37821
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:46345', name: 16, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:46345
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:41257', name: 24, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:41257
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:36983', name: 170, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:36983
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:32777', name: 131, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:32777
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:43851', name: 140, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:43851
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:40137', name: 137, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:40137
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:44923', name: 132, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:44923
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:36715', name: 143, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:36715
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:41287', name: 129, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:41287
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.3:40381', name: 89, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.3:40381
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.49.252:45421', name: 69, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.49.252:45421
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:34107', name: 128, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:34107
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.78:42245', name: 148, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.78:42245
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:46263', name: 133, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:46263
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:35215', name: 139, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:35215
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:38729', name: 141, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:38729
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:35699', name: 135, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:35699
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.192:43473', name: 37, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.192:43473
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.8.191:35419', name: 30, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.8.191:35419
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.48:43629', name: 142, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.48:43629
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.47:33037', name: 112, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.47:33037
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-8c5860f7-7ce2-11ec-aafd-79df9d7db26d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:40967', name: 164, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:40967
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:42879', name: 72, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:42879
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:41187', name: 109, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:41187
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:43245', name: 159, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:43245
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:37705', name: 50, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:37705
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:46525', name: 134, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:46525
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:39643', name: 151, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:39643
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:34287', name: 52, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:34287
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:45741', name: 160, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:45741
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:39633', name: 19, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:39633
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:35707', name: 97, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:35707
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:40019', name: 171, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:40019
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:33659', name: 39, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:33659
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:44393', name: 49, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:44393
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:34439', name: 86, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:34439
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:34251', name: 88, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:34251
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:36605', name: 145, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:36605
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:40601', name: 157, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:40601
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:41035', name: 34, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:41035
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:36447', name: 126, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:36447
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:33391', name: 82, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:33391
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:46565', name: 165, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:46565
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:36645', name: 58, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:36645
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:39003', name: 119, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:39003
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:44479', name: 55, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:44479
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:39435', name: 48, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:39435
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:36797', name: 53, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:36797
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:42165', name: 62, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:42165
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:42547', name: 93, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:42547
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:46267', name: 57, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:46267
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:35913', name: 90, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:35913
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:33341', name: 173, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:33341
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:37815', name: 105, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:37815
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:37183', name: 47, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:37183
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:37521', name: 91, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:37521
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:35959', name: 110, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:35959
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:42213', name: 96, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:42213
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:37559', name: 94, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:37559
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:36601', name: 85, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:36601
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:42651', name: 83, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:42651
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:34035', name: 101, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:34035
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:42017', name: 84, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:42017
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:38853', name: 98, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:38853
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:35721', name: 102, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:35721
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:36199', name: 95, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:36199
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:38431', name: 87, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:38431
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:41371', name: 80, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:41371
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:33231', name: 104, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:33231
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:45811', name: 107, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:45811
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:40731', name: 166, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:40731
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:42285', name: 111, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:42285
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:45477', name: 64, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:45477
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:37611', name: 172, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:37611
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:39241', name: 127, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:39241
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:43859', name: 44, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:43859
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:33495', name: 144, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:33495
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:40195', name: 150, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:40195
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:41051', name: 41, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:41051
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:41787', name: 168, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:41787
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:39335', name: 103, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:39335
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:35407', name: 43, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:35407
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:39269', name: 152, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:39269
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:36385', name: 71, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:36385
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:42841', name: 65, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:42841
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:44837', name: 68, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:44837
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:44307', name: 117, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:44307
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:35899', name: 156, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:35899
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:37365', name: 113, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:37365
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:44035', name: 125, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:44035
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:45689', name: 120, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:45689
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:34001', name: 114, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:34001
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:36655', name: 124, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:36655
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:34351', name: 78, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:34351
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:39863', name: 92, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:39863
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:44497', name: 99, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:44497
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:33775', name: 81, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:33775
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:34349', name: 35, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:34349
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:39585', name: 45, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:39585
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:36857', name: 162, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:36857
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:33587', name: 74, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:33587
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:34769', name: 32, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:34769
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:34807', name: 77, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:34807
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:32771', name: 36, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:32771
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:33967', name: 42, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:33967
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:43777', name: 136, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:43777
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:39275', name: 67, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:39275
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:41697', name: 161, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:41697
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:39383', name: 106, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:39383
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:42015', name: 146, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:42015
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:38245', name: 33, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:38245
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:39691', name: 158, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:39691
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:33507', name: 54, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:33507
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:42191', name: 66, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:42191
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:33417', name: 61, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:33417
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:45007', name: 155, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:45007
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:38959', name: 130, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:38959
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:36397', name: 63, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:36397
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:45387', name: 75, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:45387
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:38677', name: 138, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:38677
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:41103', name: 123, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:41103
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:35515', name: 116, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:35515
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:42043', name: 79, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:42043
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:39323', name: 169, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:39323
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:34761', name: 38, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:34761
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:46779', name: 28, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:46779
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:35047', name: 108, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:35047
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:32777', name: 131, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:32777
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:41287', name: 129, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:41287
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:35419', name: 30, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:35419
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:46263', name: 133, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:46263
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:40137', name: 137, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:40137
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:33165', name: 153, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:33165
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:40581', name: 25, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:40581
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:44389', name: 23, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:44389
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:40073', name: 60, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:40073
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:38113', name: 51, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:38113
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:40607', name: 31, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:40607
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:41647', name: 56, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:41647
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:35809', name: 175, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:35809
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:33115', name: 20, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:33115
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.3:40381', name: 89, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.3:40381
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:35699', name: 135, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:35699
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:43629', name: 142, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:43629
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:45421', name: 69, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:45421
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:34107', name: 128, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:34107
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:33037', name: 112, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:33037
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:40193', name: 18, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:40193
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:38729', name: 141, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:38729
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:35215', name: 139, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:35215
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:40049', name: 147, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:40049
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:41379', name: 21, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:41379
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:37239', name: 17, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:37239
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:44385', name: 70, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:44385
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:43247', name: 154, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:43247
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:33859', name: 115, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:33859
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:36715', name: 143, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:36715
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:43953', name: 76, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:43953
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:45735', name: 118, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:45735
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:36983', name: 170, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:36983
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:42235', name: 121, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:42235
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.47:37573', name: 122, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.47:37573
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:41257', name: 24, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:41257
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:37821', name: 26, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:37821
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:44923', name: 132, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:44923
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:39707', name: 27, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:39707
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:42899', name: 22, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:42899
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:40571', name: 149, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:40571
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.48:43851', name: 140, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.48:43851
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.78:42245', name: 148, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.78:42245
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:43473', name: 37, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:43473
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.251:40665', name: 59, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.251:40665
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.49.252:40431', name: 73, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.49.252:40431
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:33525', name: 167, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:33525
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - -------------------------------------------------
distributed.utils - ERROR - 'CalculateMap-698c13a8-eff6-4154-8c36-38c378da1d24'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-698c13a8-eff6-4154-8c36-38c378da1d24'
distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - -------------------------------------------------
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-698c13a8-eff6-4154-8c36-38c378da1d24'
distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - -------------------------------------------------
distributed.utils - ERROR - 'CalculateMap-e0f95c82-273e-4ea0-bcb2-a999eee5129a'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-e0f95c82-273e-4ea0-bcb2-a999eee5129a'
distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-e0f95c82-273e-4ea0-bcb2-a999eee5129a'
distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.utils - ERROR - 'CalculateMap-6d471461-1c2a-4fa9-8b3f-008176e57f1e'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-6d471461-1c2a-4fa9-8b3f-008176e57f1e'
distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-6d471461-1c2a-4fa9-8b3f-008176e57f1e'
distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.utils - ERROR - 'CalculateMap-ef6b8241-b261-47a4-8591-7b21a6c83670'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ef6b8241-b261-47a4-8591-7b21a6c83670'
distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils - ERROR - 'CalculateMap-9cf0bf77-4f29-4d6c-938a-6dc16e47f597'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-9cf0bf77-4f29-4d6c-938a-6dc16e47f597'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.utils - ERROR - 'CalculateMap-5bfc09b0-c7b2-4a5d-a151-4adcf23ccfb7'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-5bfc09b0-c7b2-4a5d-a151-4adcf23ccfb7'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils - ERROR - 'CalculateMap-fc8c925a-b059-41ef-bb1d-bb52afefee27'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-fc8c925a-b059-41ef-bb1d-bb52afefee27'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.utils - ERROR - 'CalculateMap-ec957af2-2493-4100-95bd-56093e3309fa'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ec957af2-2493-4100-95bd-56093e3309fa'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.utils - ERROR - 'CalculateMap-c9934da6-dc1c-4ec7-9ded-c384cf2d4d98'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c9934da6-dc1c-4ec7-9ded-c384cf2d4d98'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.utils - ERROR - 'CalculateMap-d3f9d8ef-93d4-4faf-b904-623266ad3261'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d3f9d8ef-93d4-4faf-b904-623266ad3261'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.utils - ERROR - 'CalculateMap-52e0834b-122f-462c-9897-78c460730b1d'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-52e0834b-122f-462c-9897-78c460730b1d'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.utils - ERROR - 'CalculateMap-f92441ea-667d-414e-93a7-68042c879360'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-f92441ea-667d-414e-93a7-68042c879360'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.utils - ERROR - 'CalculateMap-72ea2258-b541-4735-be35-adc0fabfd3d0'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-72ea2258-b541-4735-be35-adc0fabfd3d0'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.utils - ERROR - 'CalculateMap-2fb22b36-dfd3-4b40-bc89-9309a1ebcb50'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2fb22b36-dfd3-4b40-bc89-9309a1ebcb50'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ef6b8241-b261-47a4-8591-7b21a6c83670'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-9cf0bf77-4f29-4d6c-938a-6dc16e47f597'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-5bfc09b0-c7b2-4a5d-a151-4adcf23ccfb7'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-fc8c925a-b059-41ef-bb1d-bb52afefee27'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ec957af2-2493-4100-95bd-56093e3309fa'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c9934da6-dc1c-4ec7-9ded-c384cf2d4d98'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d3f9d8ef-93d4-4faf-b904-623266ad3261'
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-52e0834b-122f-462c-9897-78c460730b1d'
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-f92441ea-667d-414e-93a7-68042c879360'
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-72ea2258-b541-4735-be35-adc0fabfd3d0'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2fb22b36-dfd3-4b40-bc89-9309a1ebcb50'
distributed.worker - INFO - -------------------------------------------------
distributed.utils - ERROR - 'CalculateMap-d43fc258-d54b-48f7-a82f-9e78927596df'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d43fc258-d54b-48f7-a82f-9e78927596df'
distributed.worker - INFO - -------------------------------------------------
distributed.utils - ERROR - 'CalculateMap-d363d217-37c6-447e-afa3-d1da70ad7387'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d363d217-37c6-447e-afa3-d1da70ad7387'
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.utils - ERROR - 'CalculateMap-5a5ea280-a3fc-406d-8240-e1e0ae4f5990'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-5a5ea280-a3fc-406d-8240-e1e0ae4f5990'
distributed.worker - INFO - -------------------------------------------------
distributed.utils - ERROR - 'CalculateMap-b90d8079-61e4-445e-a5f9-5895b1744e23'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-b90d8079-61e4-445e-a5f9-5895b1744e23'
distributed.core - INFO - Starting established connection
distributed.utils - ERROR - 'CalculateMap-3692d512-b3e3-4f50-a5e9-7e4f85e2697f'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3692d512-b3e3-4f50-a5e9-7e4f85e2697f'
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.utils - ERROR - 'CalculateMap-7f055ff0-889f-42b3-8dd7-9e54202db0c9'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-7f055ff0-889f-42b3-8dd7-9e54202db0c9'
distributed.worker - INFO - -------------------------------------------------
distributed.utils - ERROR - 'CalculateMap-723cad08-6a26-48fe-aae1-dd419b641850'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-723cad08-6a26-48fe-aae1-dd419b641850'
distributed.core - INFO - Starting established connection
distributed.utils - ERROR - 'CalculateMap-bcecaa9f-c172-4adc-96a0-a82d4c259ddf'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-bcecaa9f-c172-4adc-96a0-a82d4c259ddf'
distributed.utils - ERROR - 'CalculateMap-1ee3201b-b47a-4be4-ae17-9f87a07e0cb2'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1ee3201b-b47a-4be4-ae17-9f87a07e0cb2'
distributed.utils - ERROR - 'CalculateMap-478b6ae4-e235-4409-b1ce-ee35453438ca'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-478b6ae4-e235-4409-b1ce-ee35453438ca'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d43fc258-d54b-48f7-a82f-9e78927596df'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d363d217-37c6-447e-afa3-d1da70ad7387'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-5a5ea280-a3fc-406d-8240-e1e0ae4f5990'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-b90d8079-61e4-445e-a5f9-5895b1744e23'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3692d512-b3e3-4f50-a5e9-7e4f85e2697f'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-7f055ff0-889f-42b3-8dd7-9e54202db0c9'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-723cad08-6a26-48fe-aae1-dd419b641850'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-bcecaa9f-c172-4adc-96a0-a82d4c259ddf'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1ee3201b-b47a-4be4-ae17-9f87a07e0cb2'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-478b6ae4-e235-4409-b1ce-ee35453438ca'
distributed.utils - ERROR - 'CalculateMap-c435b619-50e2-4045-a368-d96b9ccadd6e'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c435b619-50e2-4045-a368-d96b9ccadd6e'
distributed.utils - ERROR - 'CalculateMap-ac1b4c8c-85e0-4f76-bbc2-90f2cbfa5d7a'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ac1b4c8c-85e0-4f76-bbc2-90f2cbfa5d7a'
distributed.utils - ERROR - 'CalculateMap-78448be1-a8eb-4bd1-a1ff-12748d175524'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-78448be1-a8eb-4bd1-a1ff-12748d175524'
distributed.utils - ERROR - 'CalculateMap-057596e8-fe14-4735-85e5-2cc714e31788'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-057596e8-fe14-4735-85e5-2cc714e31788'
distributed.utils - ERROR - 'CalculateMap-b118ac12-2b3f-4d7e-8a81-a305991834d0'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-b118ac12-2b3f-4d7e-8a81-a305991834d0'
distributed.utils - ERROR - 'CalculateMap-1cfbe497-cba7-4220-9978-0c3408240ce8'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1cfbe497-cba7-4220-9978-0c3408240ce8'
distributed.utils - ERROR - 'CalculateMap-87d94001-15a7-4ab1-8e81-7bee9ccd89a1'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-87d94001-15a7-4ab1-8e81-7bee9ccd89a1'
distributed.utils - ERROR - 'CalculateMap-92f90249-0959-45a6-88b2-afc6bec65f05'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-92f90249-0959-45a6-88b2-afc6bec65f05'
distributed.utils - ERROR - 'CalculateMap-6fad607a-9ce1-471e-a4b8-318fcbe00cea'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-6fad607a-9ce1-471e-a4b8-318fcbe00cea'
distributed.utils - ERROR - 'CalculateMap-ff555414-a9e9-4918-8638-4d5c06a979fe'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ff555414-a9e9-4918-8638-4d5c06a979fe'
distributed.utils - ERROR - 'CalculateMap-4f64a0ee-e651-4c4b-af7c-74a035dc1519'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-4f64a0ee-e651-4c4b-af7c-74a035dc1519'
distributed.utils - ERROR - 'CalculateMap-136271af-d645-4dad-b2da-2386b2d7d64b'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-136271af-d645-4dad-b2da-2386b2d7d64b'
distributed.utils - ERROR - 'CalculateMap-24d4a0e7-89c3-40df-a9d4-98f341e81bdc'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-24d4a0e7-89c3-40df-a9d4-98f341e81bdc'
distributed.utils - ERROR - 'CalculateMap-87f349b2-6fbc-4cda-b23e-28ef63048332'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-87f349b2-6fbc-4cda-b23e-28ef63048332'
distributed.utils - ERROR - 'CalculateMap-48dd38c3-fc3a-4a1c-bae5-ae6036a63465'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-48dd38c3-fc3a-4a1c-bae5-ae6036a63465'
distributed.utils - ERROR - 'CalculateMap-93b90d0b-0d1c-4762-9ac2-094483a00de4'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-93b90d0b-0d1c-4762-9ac2-094483a00de4'
distributed.utils - ERROR - 'CalculateMap-beaf5320-f460-44d2-943e-80a62b9cf82d'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-beaf5320-f460-44d2-943e-80a62b9cf82d'
distributed.utils - ERROR - 'CalculateMap-14e15212-6acc-47ec-863e-141f71eb414f'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-14e15212-6acc-47ec-863e-141f71eb414f'
distributed.utils - ERROR - 'CalculateMap-c8fece4c-22c2-4015-ad80-f94510bb5442'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c8fece4c-22c2-4015-ad80-f94510bb5442'
distributed.utils - ERROR - 'CalculateMap-aed6b38c-5d88-44d5-bea2-0f65e6bc3956'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-aed6b38c-5d88-44d5-bea2-0f65e6bc3956'
distributed.utils - ERROR - 'CalculateMap-18fc8585-cd1d-48ab-8564-471c7d6b6a95'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-18fc8585-cd1d-48ab-8564-471c7d6b6a95'
distributed.utils - ERROR - 'CalculateMap-c5fa0c49-3188-4a30-a5dc-ce267c43adf9'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c5fa0c49-3188-4a30-a5dc-ce267c43adf9'
distributed.utils - ERROR - 'CalculateMap-0df39e35-c506-4765-b615-4a633dbc9aa4'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-0df39e35-c506-4765-b615-4a633dbc9aa4'
distributed.utils - ERROR - 'CalculateMap-e741b045-f106-4bcb-93ce-3137ad45986b'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-e741b045-f106-4bcb-93ce-3137ad45986b'
distributed.utils - ERROR - 'CalculateMap-586d5be8-2d96-49b8-a4a7-0c028abaa8a0'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-586d5be8-2d96-49b8-a4a7-0c028abaa8a0'
distributed.utils - ERROR - 'CalculateMap-051eef28-2d99-40e8-b0a6-391c0eeef7c3'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-051eef28-2d99-40e8-b0a6-391c0eeef7c3'
distributed.utils - ERROR - 'CalculateMap-db5ef9b7-9852-49d9-8d40-5e8a7e989352'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-db5ef9b7-9852-49d9-8d40-5e8a7e989352'
distributed.utils - ERROR - 'CalculateMap-afe5455f-a748-48ba-bf22-b41b923596f0'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-afe5455f-a748-48ba-bf22-b41b923596f0'
distributed.utils - ERROR - 'CalculateMap-285ba26d-f6a0-45d3-aa95-8d8a09ecc659'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-285ba26d-f6a0-45d3-aa95-8d8a09ecc659'
distributed.utils - ERROR - 'CalculateMap-07ad2671-4911-4ffc-bb0c-24230136b915'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-07ad2671-4911-4ffc-bb0c-24230136b915'
distributed.utils - ERROR - 'CalculateMap-afe5633f-ec28-4056-a4ea-ca6961e93e19'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-afe5633f-ec28-4056-a4ea-ca6961e93e19'
distributed.utils - ERROR - 'CalculateMap-111d2212-a4d2-4087-aff8-e250437cc314'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-111d2212-a4d2-4087-aff8-e250437cc314'
distributed.utils - ERROR - 'CalculateMap-45aa9587-fa47-41d6-8551-6ea34230fd4a'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-45aa9587-fa47-41d6-8551-6ea34230fd4a'
distributed.utils - ERROR - 'CalculateMap-bf1b86f4-bd42-4d71-b707-7cd9384937c3'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-bf1b86f4-bd42-4d71-b707-7cd9384937c3'
distributed.utils - ERROR - 'CalculateMap-80caddfc-e8cd-4de7-8e91-49c8b464e85c'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-80caddfc-e8cd-4de7-8e91-49c8b464e85c'
distributed.utils - ERROR - 'CalculateMap-3e4a15ef-87cf-421b-8ba7-932c70513334'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3e4a15ef-87cf-421b-8ba7-932c70513334'
distributed.utils - ERROR - 'CalculateMap-4cfb4e0d-771e-4bea-9ee4-a1280888d790'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-4cfb4e0d-771e-4bea-9ee4-a1280888d790'
distributed.utils - ERROR - 'CalculateMap-457e0b80-981a-410f-8a61-bd0c93546b0a'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-457e0b80-981a-410f-8a61-bd0c93546b0a'
distributed.utils - ERROR - 'CalculateMap-858515cd-7684-49b4-beb1-3c9f94aca80f'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-858515cd-7684-49b4-beb1-3c9f94aca80f'
distributed.utils - ERROR - 'CalculateMap-3bc197b2-0ce4-43fe-8ce0-a715a45efec9'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3bc197b2-0ce4-43fe-8ce0-a715a45efec9'
distributed.utils - ERROR - 'CalculateMap-fced0d77-652d-4c62-a033-db15e460cf40'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-fced0d77-652d-4c62-a033-db15e460cf40'
distributed.utils - ERROR - 'CalculateMap-d0e48cf3-fd1f-4a8d-b184-75b6221b28a4'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d0e48cf3-fd1f-4a8d-b184-75b6221b28a4'
distributed.utils - ERROR - 'CalculateMap-3e1e127b-d8ba-4f70-9252-43b9a6307828'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3e1e127b-d8ba-4f70-9252-43b9a6307828'
distributed.utils - ERROR - 'CalculateMap-51bae3f0-bedf-4cd7-83e8-317d7c16824a'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-51bae3f0-bedf-4cd7-83e8-317d7c16824a'
distributed.utils - ERROR - 'CalculateMap-1fc0f030-0a92-4d78-96d9-8bdbc347485d'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1fc0f030-0a92-4d78-96d9-8bdbc347485d'
distributed.utils - ERROR - 'CalculateMap-1ddfe2e7-c428-4684-988a-6949e63520e6'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1ddfe2e7-c428-4684-988a-6949e63520e6'
distributed.utils - ERROR - 'CalculateMap-76aac383-3b0f-4d08-9ebf-00b7cd1a1ec7'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-76aac383-3b0f-4d08-9ebf-00b7cd1a1ec7'
distributed.utils - ERROR - 'CalculateMap-e9d5fd6d-1746-435d-aae2-2b53fb7ff521'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-e9d5fd6d-1746-435d-aae2-2b53fb7ff521'
distributed.utils - ERROR - 'CalculateMap-36163452-9c65-49d4-85f1-a246a3bc363a'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-36163452-9c65-49d4-85f1-a246a3bc363a'
distributed.utils - ERROR - 'CalculateMap-0a4d9ea2-69f6-4973-8659-bb578a9b18a9'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-0a4d9ea2-69f6-4973-8659-bb578a9b18a9'
distributed.utils - ERROR - 'CalculateMap-def01828-55f8-4e0c-844f-15093998a556'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-def01828-55f8-4e0c-844f-15093998a556'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c435b619-50e2-4045-a368-d96b9ccadd6e'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ac1b4c8c-85e0-4f76-bbc2-90f2cbfa5d7a'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-78448be1-a8eb-4bd1-a1ff-12748d175524'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-057596e8-fe14-4735-85e5-2cc714e31788'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-b118ac12-2b3f-4d7e-8a81-a305991834d0'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1cfbe497-cba7-4220-9978-0c3408240ce8'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-87d94001-15a7-4ab1-8e81-7bee9ccd89a1'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-92f90249-0959-45a6-88b2-afc6bec65f05'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-6fad607a-9ce1-471e-a4b8-318fcbe00cea'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ff555414-a9e9-4918-8638-4d5c06a979fe'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-4f64a0ee-e651-4c4b-af7c-74a035dc1519'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-136271af-d645-4dad-b2da-2386b2d7d64b'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-24d4a0e7-89c3-40df-a9d4-98f341e81bdc'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-87f349b2-6fbc-4cda-b23e-28ef63048332'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-48dd38c3-fc3a-4a1c-bae5-ae6036a63465'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-93b90d0b-0d1c-4762-9ac2-094483a00de4'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-beaf5320-f460-44d2-943e-80a62b9cf82d'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-14e15212-6acc-47ec-863e-141f71eb414f'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c8fece4c-22c2-4015-ad80-f94510bb5442'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-aed6b38c-5d88-44d5-bea2-0f65e6bc3956'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-18fc8585-cd1d-48ab-8564-471c7d6b6a95'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c5fa0c49-3188-4a30-a5dc-ce267c43adf9'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-0df39e35-c506-4765-b615-4a633dbc9aa4'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-e741b045-f106-4bcb-93ce-3137ad45986b'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-586d5be8-2d96-49b8-a4a7-0c028abaa8a0'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-051eef28-2d99-40e8-b0a6-391c0eeef7c3'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-db5ef9b7-9852-49d9-8d40-5e8a7e989352'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-afe5455f-a748-48ba-bf22-b41b923596f0'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-285ba26d-f6a0-45d3-aa95-8d8a09ecc659'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-07ad2671-4911-4ffc-bb0c-24230136b915'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-afe5633f-ec28-4056-a4ea-ca6961e93e19'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-111d2212-a4d2-4087-aff8-e250437cc314'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-45aa9587-fa47-41d6-8551-6ea34230fd4a'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-bf1b86f4-bd42-4d71-b707-7cd9384937c3'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-80caddfc-e8cd-4de7-8e91-49c8b464e85c'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3e4a15ef-87cf-421b-8ba7-932c70513334'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-4cfb4e0d-771e-4bea-9ee4-a1280888d790'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-457e0b80-981a-410f-8a61-bd0c93546b0a'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-858515cd-7684-49b4-beb1-3c9f94aca80f'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3bc197b2-0ce4-43fe-8ce0-a715a45efec9'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-fced0d77-652d-4c62-a033-db15e460cf40'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d0e48cf3-fd1f-4a8d-b184-75b6221b28a4'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3e1e127b-d8ba-4f70-9252-43b9a6307828'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-51bae3f0-bedf-4cd7-83e8-317d7c16824a'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1fc0f030-0a92-4d78-96d9-8bdbc347485d'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1ddfe2e7-c428-4684-988a-6949e63520e6'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-76aac383-3b0f-4d08-9ebf-00b7cd1a1ec7'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-e9d5fd6d-1746-435d-aae2-2b53fb7ff521'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-36163452-9c65-49d4-85f1-a246a3bc363a'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-0a4d9ea2-69f6-4973-8659-bb578a9b18a9'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-def01828-55f8-4e0c-844f-15093998a556'
distributed.utils - ERROR - 'CalculateMap-323e2fb4-b3ac-422e-82e1-f6ca8a976448'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-323e2fb4-b3ac-422e-82e1-f6ca8a976448'
distributed.utils - ERROR - 'CalculateMap-6516e0b6-c2fc-4996-8027-3b383009044e'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-6516e0b6-c2fc-4996-8027-3b383009044e'
distributed.utils - ERROR - 'CalculateMap-96969fd0-0abd-4ef6-bb11-f3e15f8ba4a2'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-96969fd0-0abd-4ef6-bb11-f3e15f8ba4a2'
distributed.utils - ERROR - 'CalculateMap-e2db4c69-80e7-462a-a4a5-0255d77139bf'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-e2db4c69-80e7-462a-a4a5-0255d77139bf'
distributed.utils - ERROR - 'CalculateMap-023f72da-b827-4f8e-8e87-ae794d61b8c7'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-023f72da-b827-4f8e-8e87-ae794d61b8c7'
distributed.utils - ERROR - 'CalculateMap-c138ebf5-51f1-4cf2-aeb9-e03091c27ae0'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c138ebf5-51f1-4cf2-aeb9-e03091c27ae0'
distributed.utils - ERROR - 'CalculateMap-539d00f2-21c3-4603-9c5a-27c8529bf391'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-539d00f2-21c3-4603-9c5a-27c8529bf391'
distributed.utils - ERROR - 'CalculateMap-3ad8709e-687e-4e04-b1cb-3000cb416a06'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3ad8709e-687e-4e04-b1cb-3000cb416a06'
distributed.utils - ERROR - 'CalculateMap-6c31cdae-4046-4502-9b39-bd2ae1b22a6e'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-6c31cdae-4046-4502-9b39-bd2ae1b22a6e'
distributed.utils - ERROR - 'CalculateMap-072197da-1f42-4869-a62c-d6f1fd2d3758'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-072197da-1f42-4869-a62c-d6f1fd2d3758'
distributed.utils - ERROR - 'CalculateMap-da477ecf-d9c5-469f-8198-401896fa3d43'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-da477ecf-d9c5-469f-8198-401896fa3d43'
distributed.utils - ERROR - 'CalculateMap-1937bf2d-8ec9-4781-9e56-57ff697c91ba'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1937bf2d-8ec9-4781-9e56-57ff697c91ba'
distributed.utils - ERROR - 'CalculateMap-9c202535-c493-410c-8a9b-1a636ffa3ac3'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-9c202535-c493-410c-8a9b-1a636ffa3ac3'
distributed.utils - ERROR - 'CalculateMap-c6943692-ed4b-4c89-bcba-8d9f4c7e0499'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c6943692-ed4b-4c89-bcba-8d9f4c7e0499'
distributed.utils - ERROR - 'CalculateMap-6ba78b53-5147-46a6-96a6-9b1d08d361d4'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-6ba78b53-5147-46a6-96a6-9b1d08d361d4'
distributed.utils - ERROR - 'CalculateMap-398d777a-e41b-4369-a49c-b76fba17d7c9'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-398d777a-e41b-4369-a49c-b76fba17d7c9'
distributed.utils - ERROR - 'CalculateMap-f5fdf269-d42b-49e2-aee4-16f5ef4bc00c'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-f5fdf269-d42b-49e2-aee4-16f5ef4bc00c'
distributed.utils - ERROR - 'CalculateMap-d90da2c4-7586-4c20-930c-a29a9c50cd78'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d90da2c4-7586-4c20-930c-a29a9c50cd78'
distributed.utils - ERROR - 'CalculateMap-ad62b5b9-c13a-4439-b9b8-ee47552a2efb'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ad62b5b9-c13a-4439-b9b8-ee47552a2efb'
distributed.utils - ERROR - 'CalculateMap-242c32ff-b764-4bbb-87b5-bffdf07faa27'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-242c32ff-b764-4bbb-87b5-bffdf07faa27'
distributed.utils - ERROR - 'CalculateMap-1394f92f-7981-4f45-93d5-36c51d141648'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1394f92f-7981-4f45-93d5-36c51d141648'
distributed.utils - ERROR - 'CalculateMap-68618673-08f4-4b87-b1ed-69a0005bfc07'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-68618673-08f4-4b87-b1ed-69a0005bfc07'
distributed.utils - ERROR - 'CalculateMap-80e7c97e-39ef-4705-b643-deae72bc4ee7'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-80e7c97e-39ef-4705-b643-deae72bc4ee7'
distributed.utils - ERROR - 'CalculateMap-57c39178-dcea-443c-b148-0abb9d274839'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-57c39178-dcea-443c-b148-0abb9d274839'
distributed.utils - ERROR - 'CalculateMap-5651ad46-f185-4c5e-84a7-953f07bc8623'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-5651ad46-f185-4c5e-84a7-953f07bc8623'
distributed.utils - ERROR - 'CalculateMap-ea4af920-3e98-43bb-925e-37657d4ba12a'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ea4af920-3e98-43bb-925e-37657d4ba12a'
distributed.utils - ERROR - 'CalculateMap-1792fa19-5b7b-4a48-b0d0-197dbc0da6d6'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1792fa19-5b7b-4a48-b0d0-197dbc0da6d6'
distributed.utils - ERROR - 'CalculateMap-2eefdc5f-238f-47c0-b351-defe9eb585ed'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2eefdc5f-238f-47c0-b351-defe9eb585ed'
distributed.utils - ERROR - 'CalculateMap-0e4aff08-cb50-4c8d-99d7-14356677ce5b'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-0e4aff08-cb50-4c8d-99d7-14356677ce5b'
distributed.utils - ERROR - 'CalculateMap-e8143704-4f11-47a9-b61e-4162998d8624'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-e8143704-4f11-47a9-b61e-4162998d8624'
distributed.utils - ERROR - 'CalculateMap-dc79e8ae-c37f-41d3-aec1-a6d4baf9c0be'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-dc79e8ae-c37f-41d3-aec1-a6d4baf9c0be'
distributed.utils - ERROR - 'CalculateMap-2760d894-d1b6-44c2-9d8c-2b600cf8cc01'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2760d894-d1b6-44c2-9d8c-2b600cf8cc01'
distributed.utils - ERROR - 'CalculateMap-4872a70c-359a-4812-97ed-a7dcd7ee68b6'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-4872a70c-359a-4812-97ed-a7dcd7ee68b6'
distributed.utils - ERROR - 'CalculateMap-1af5a98d-75e0-47fe-b372-939c7cacdcb0'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1af5a98d-75e0-47fe-b372-939c7cacdcb0'
distributed.utils - ERROR - 'CalculateMap-d85913bc-44cc-47e7-a313-42b06dc607f7'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d85913bc-44cc-47e7-a313-42b06dc607f7'
distributed.utils - ERROR - 'CalculateMap-71fca6de-e7d4-44cb-94e5-65c768defeec'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-71fca6de-e7d4-44cb-94e5-65c768defeec'
distributed.utils - ERROR - 'CalculateMap-dcd0d2e3-6df0-4e9a-b590-33e10c90c117'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-dcd0d2e3-6df0-4e9a-b590-33e10c90c117'
distributed.utils - ERROR - 'CalculateMap-7f72b3eb-ef20-452e-9fc4-aa2e34dfd845'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-7f72b3eb-ef20-452e-9fc4-aa2e34dfd845'
distributed.utils - ERROR - 'CalculateMap-5a33e431-c08f-47a8-8692-95a4368e0f2a'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-5a33e431-c08f-47a8-8692-95a4368e0f2a'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-323e2fb4-b3ac-422e-82e1-f6ca8a976448'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-6516e0b6-c2fc-4996-8027-3b383009044e'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-96969fd0-0abd-4ef6-bb11-f3e15f8ba4a2'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-e2db4c69-80e7-462a-a4a5-0255d77139bf'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-023f72da-b827-4f8e-8e87-ae794d61b8c7'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c138ebf5-51f1-4cf2-aeb9-e03091c27ae0'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-539d00f2-21c3-4603-9c5a-27c8529bf391'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3ad8709e-687e-4e04-b1cb-3000cb416a06'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-6c31cdae-4046-4502-9b39-bd2ae1b22a6e'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-072197da-1f42-4869-a62c-d6f1fd2d3758'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-da477ecf-d9c5-469f-8198-401896fa3d43'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1937bf2d-8ec9-4781-9e56-57ff697c91ba'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-9c202535-c493-410c-8a9b-1a636ffa3ac3'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c6943692-ed4b-4c89-bcba-8d9f4c7e0499'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-6ba78b53-5147-46a6-96a6-9b1d08d361d4'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-398d777a-e41b-4369-a49c-b76fba17d7c9'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-f5fdf269-d42b-49e2-aee4-16f5ef4bc00c'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d90da2c4-7586-4c20-930c-a29a9c50cd78'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ad62b5b9-c13a-4439-b9b8-ee47552a2efb'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-242c32ff-b764-4bbb-87b5-bffdf07faa27'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1394f92f-7981-4f45-93d5-36c51d141648'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-68618673-08f4-4b87-b1ed-69a0005bfc07'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-80e7c97e-39ef-4705-b643-deae72bc4ee7'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-57c39178-dcea-443c-b148-0abb9d274839'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-5651ad46-f185-4c5e-84a7-953f07bc8623'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ea4af920-3e98-43bb-925e-37657d4ba12a'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1792fa19-5b7b-4a48-b0d0-197dbc0da6d6'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2eefdc5f-238f-47c0-b351-defe9eb585ed'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-0e4aff08-cb50-4c8d-99d7-14356677ce5b'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-e8143704-4f11-47a9-b61e-4162998d8624'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-dc79e8ae-c37f-41d3-aec1-a6d4baf9c0be'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2760d894-d1b6-44c2-9d8c-2b600cf8cc01'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-4872a70c-359a-4812-97ed-a7dcd7ee68b6'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1af5a98d-75e0-47fe-b372-939c7cacdcb0'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d85913bc-44cc-47e7-a313-42b06dc607f7'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-71fca6de-e7d4-44cb-94e5-65c768defeec'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-dcd0d2e3-6df0-4e9a-b590-33e10c90c117'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-7f72b3eb-ef20-452e-9fc4-aa2e34dfd845'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-5a33e431-c08f-47a8-8692-95a4368e0f2a'
distributed.utils - ERROR - 'CalculateMap-31c9dda8-a372-4826-8df4-73994f41ac13'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-31c9dda8-a372-4826-8df4-73994f41ac13'
distributed.utils - ERROR - 'CalculateMap-d388b1e0-3485-4c7d-a739-981661253aaf'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d388b1e0-3485-4c7d-a739-981661253aaf'
distributed.utils - ERROR - 'CalculateMap-279c4db7-c341-4e40-a213-98ddbb1ad8f7'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-279c4db7-c341-4e40-a213-98ddbb1ad8f7'
distributed.utils - ERROR - 'CalculateMap-807fbafc-0b5a-4a49-935d-2ed164a03605'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-807fbafc-0b5a-4a49-935d-2ed164a03605'
distributed.utils - ERROR - 'CalculateMap-bfb1b7c9-9b1c-44c7-b185-8aa3169d5a43'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-bfb1b7c9-9b1c-44c7-b185-8aa3169d5a43'
distributed.utils - ERROR - 'CalculateMap-d2b0e1b9-a365-404c-8cce-eed7a6c76c41'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d2b0e1b9-a365-404c-8cce-eed7a6c76c41'
distributed.utils - ERROR - 'CalculateMap-c90994a1-f04d-4085-a2c5-a398f49d0a71'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c90994a1-f04d-4085-a2c5-a398f49d0a71'
distributed.utils - ERROR - 'CalculateMap-d6b1ef6e-2720-46bb-ad04-8620fe21b2d9'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d6b1ef6e-2720-46bb-ad04-8620fe21b2d9'
distributed.utils - ERROR - 'CalculateMap-3ff57cac-f905-4d7a-aad1-bb9cbbb45dd8'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3ff57cac-f905-4d7a-aad1-bb9cbbb45dd8'
distributed.utils - ERROR - 'CalculateMap-2fc8ce38-6e60-4737-a02f-1327703880ed'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2fc8ce38-6e60-4737-a02f-1327703880ed'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-31c9dda8-a372-4826-8df4-73994f41ac13'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d388b1e0-3485-4c7d-a739-981661253aaf'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-279c4db7-c341-4e40-a213-98ddbb1ad8f7'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-807fbafc-0b5a-4a49-935d-2ed164a03605'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-bfb1b7c9-9b1c-44c7-b185-8aa3169d5a43'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d2b0e1b9-a365-404c-8cce-eed7a6c76c41'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-c90994a1-f04d-4085-a2c5-a398f49d0a71'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d6b1ef6e-2720-46bb-ad04-8620fe21b2d9'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-3ff57cac-f905-4d7a-aad1-bb9cbbb45dd8'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2fc8ce38-6e60-4737-a02f-1327703880ed'
distributed.utils - ERROR - 'CalculateMap-4f6de366-a1d1-4ea3-a8e9-b7b12a2573a5'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-4f6de366-a1d1-4ea3-a8e9-b7b12a2573a5'
distributed.utils - ERROR - 'CalculateMap-79ef9b11-ee50-4769-a41d-b6c9567ecbf0'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-79ef9b11-ee50-4769-a41d-b6c9567ecbf0'
distributed.utils - ERROR - 'CalculateMap-234405a2-953b-49cb-a47f-72a2de9c8f86'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-234405a2-953b-49cb-a47f-72a2de9c8f86'
distributed.utils - ERROR - 'CalculateMap-b0eae0c6-fd6b-415c-a22b-f32badfe9004'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-b0eae0c6-fd6b-415c-a22b-f32badfe9004'
distributed.utils - ERROR - 'CalculateMap-1d4f9be2-f664-437c-8440-6c74efc9db1e'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1d4f9be2-f664-437c-8440-6c74efc9db1e'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-4f6de366-a1d1-4ea3-a8e9-b7b12a2573a5'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-79ef9b11-ee50-4769-a41d-b6c9567ecbf0'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-234405a2-953b-49cb-a47f-72a2de9c8f86'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-b0eae0c6-fd6b-415c-a22b-f32badfe9004'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1d4f9be2-f664-437c-8440-6c74efc9db1e'
distributed.utils - ERROR - 'CalculateMap-a51a7f15-2b32-4010-b11e-13ebadc5dc39'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-a51a7f15-2b32-4010-b11e-13ebadc5dc39'
distributed.utils - ERROR - 'CalculateMap-2cbbc158-c4fc-4026-80bc-934a771acbf6'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2cbbc158-c4fc-4026-80bc-934a771acbf6'
distributed.utils - ERROR - 'CalculateMap-197399bd-857d-43c2-a565-9faefbfc0abb'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-197399bd-857d-43c2-a565-9faefbfc0abb'
distributed.utils - ERROR - 'CalculateMap-ee41d92d-4365-4430-9755-c002d79538d5'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ee41d92d-4365-4430-9755-c002d79538d5'
distributed.utils - ERROR - 'CalculateMap-f6399e2d-13ca-4eeb-9f47-e887f3e48621'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-f6399e2d-13ca-4eeb-9f47-e887f3e48621'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-a51a7f15-2b32-4010-b11e-13ebadc5dc39'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2cbbc158-c4fc-4026-80bc-934a771acbf6'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-197399bd-857d-43c2-a565-9faefbfc0abb'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ee41d92d-4365-4430-9755-c002d79538d5'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-f6399e2d-13ca-4eeb-9f47-e887f3e48621'
distributed.utils - ERROR - 'CalculateMap-fe8eca19-b543-4db8-89b2-21625419899f'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-fe8eca19-b543-4db8-89b2-21625419899f'
distributed.utils - ERROR - 'CalculateMap-de3cf79d-127d-46bc-8698-85d6d8861df5'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-de3cf79d-127d-46bc-8698-85d6d8861df5'
distributed.utils - ERROR - 'CalculateMap-219d4ea5-7e0f-4337-92ed-ce5cfce96ffb'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-219d4ea5-7e0f-4337-92ed-ce5cfce96ffb'
distributed.utils - ERROR - 'CalculateMap-f5df987c-194d-4e6d-8181-aaf2d1a20c41'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-f5df987c-194d-4e6d-8181-aaf2d1a20c41'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-fe8eca19-b543-4db8-89b2-21625419899f'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-de3cf79d-127d-46bc-8698-85d6d8861df5'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-219d4ea5-7e0f-4337-92ed-ce5cfce96ffb'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-f5df987c-194d-4e6d-8181-aaf2d1a20c41'
distributed.utils - ERROR - 'CalculateMap-d813a169-f162-4324-a8b4-899d107a6c97'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d813a169-f162-4324-a8b4-899d107a6c97'
distributed.utils - ERROR - 'CalculateMap-5e7f2af6-e0bb-4f05-bb26-cee328e1aef4'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-5e7f2af6-e0bb-4f05-bb26-cee328e1aef4'
distributed.utils - ERROR - 'CalculateMap-38d7613d-866a-46aa-8bde-1cd72bfa313e'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-38d7613d-866a-46aa-8bde-1cd72bfa313e'
distributed.utils - ERROR - 'CalculateMap-03b976f4-d491-489a-9a10-5ca0aedd2dd6'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-03b976f4-d491-489a-9a10-5ca0aedd2dd6'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-d813a169-f162-4324-a8b4-899d107a6c97'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-5e7f2af6-e0bb-4f05-bb26-cee328e1aef4'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-38d7613d-866a-46aa-8bde-1cd72bfa313e'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-03b976f4-d491-489a-9a10-5ca0aedd2dd6'
distributed.utils - ERROR - 'CalculateMap-73f5cfba-ae14-4ac5-93e5-902d19e42f1d'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-73f5cfba-ae14-4ac5-93e5-902d19e42f1d'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-73f5cfba-ae14-4ac5-93e5-902d19e42f1d'
distributed.utils - ERROR - 'CalculateMap-dfadb3b5-e4f8-4e0d-ba35-a755e97e0efe'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-dfadb3b5-e4f8-4e0d-ba35-a755e97e0efe'
distributed.utils - ERROR - 'CalculateMap-38166967-3799-486e-abdd-f5fc10126f87'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-38166967-3799-486e-abdd-f5fc10126f87'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-dfadb3b5-e4f8-4e0d-ba35-a755e97e0efe'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-38166967-3799-486e-abdd-f5fc10126f87'
distributed.utils - ERROR - 'CalculateMap-f2923e46-90b9-457e-98f0-a3c1e64dbc1f'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-f2923e46-90b9-457e-98f0-a3c1e64dbc1f'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-f2923e46-90b9-457e-98f0-a3c1e64dbc1f'
distributed.utils - ERROR - 'CalculateMap-b27214cd-b795-4149-b4d4-a7d702191762'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-b27214cd-b795-4149-b4d4-a7d702191762'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-b27214cd-b795-4149-b4d4-a7d702191762'
distributed.utils - ERROR - 'CalculateMap-21681b4d-7f98-444e-9e1e-0d3f71105efe'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-21681b4d-7f98-444e-9e1e-0d3f71105efe'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-21681b4d-7f98-444e-9e1e-0d3f71105efe'
distributed.utils - ERROR - 'CalculateMap-69ac407f-f4eb-40d0-aced-5b9948d2277a'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-69ac407f-f4eb-40d0-aced-5b9948d2277a'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-69ac407f-f4eb-40d0-aced-5b9948d2277a'
distributed.utils - ERROR - 'CalculateMap-1b34039d-f355-4ead-ba9f-a6d460f50f79'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1b34039d-f355-4ead-ba9f-a6d460f50f79'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1b34039d-f355-4ead-ba9f-a6d460f50f79'
distributed.utils - ERROR - 'CalculateMap-29b67f12-d45f-4325-9241-98bd345669f8'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-29b67f12-d45f-4325-9241-98bd345669f8'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-29b67f12-d45f-4325-9241-98bd345669f8'
distributed.utils - ERROR - 'CalculateMap-ea55b9c5-b235-4b17-99e9-7c4e66f67d4a'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ea55b9c5-b235-4b17-99e9-7c4e66f67d4a'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ea55b9c5-b235-4b17-99e9-7c4e66f67d4a'
distributed.utils - ERROR - 'CalculateMap-18c93786-53a5-44d2-891b-fc25f247cec6'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-18c93786-53a5-44d2-891b-fc25f247cec6'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-18c93786-53a5-44d2-891b-fc25f247cec6'
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:36049', name: 163, memory: 0, processing: 22>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:36049
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:33179', name: 174, memory: 0, processing: 22>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:33179
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:45151', name: 100, memory: 0, processing: 22>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:45151
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:34457', name: 46, memory: 0, processing: 22>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:34457
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:46345', name: 16, memory: 0, processing: 22>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:46345
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:33179', name: 174, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:33179
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:36049', name: 163, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:36049
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:45151', name: 100, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:45151
distributed.core - INFO - Starting established connection
distributed.utils - ERROR - 'CalculateMap-ad62b5b9-c13a-4439-b9b8-ee47552a2efb'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ad62b5b9-c13a-4439-b9b8-ee47552a2efb'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-ad62b5b9-c13a-4439-b9b8-ee47552a2efb'
distributed.utils - ERROR - 'CalculateMap-136271af-d645-4dad-b2da-2386b2d7d64b'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-136271af-d645-4dad-b2da-2386b2d7d64b'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-136271af-d645-4dad-b2da-2386b2d7d64b'
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.192:36273', name: 40, memory: 0, processing: 21>
distributed.core - INFO - Removing comms to tcp://10.128.8.192:36273
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.8.191:37117', name: 29, memory: 0, processing: 23>
distributed.core - INFO - Removing comms to tcp://10.128.8.191:37117
distributed.utils - ERROR - 'CalculateMap-1ddfe2e7-c428-4684-988a-6949e63520e6'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1ddfe2e7-c428-4684-988a-6949e63520e6'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-1ddfe2e7-c428-4684-988a-6949e63520e6'
distributed.utils - ERROR - 'CalculateMap-2cbbc158-c4fc-4026-80bc-934a771acbf6'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2cbbc158-c4fc-4026-80bc-934a771acbf6'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-2cbbc158-c4fc-4026-80bc-934a771acbf6'
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:33179', name: 174, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:33179
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:45151', name: 100, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:45151
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:36049', name: 163, memory: 0, processing: 1>
distributed.core - INFO - Removing comms to tcp://10.128.50.79:36049
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:34251', name: 88, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:34251', name: 88, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:34251', name: 88, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:36601', name: 85, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:36601', name: 85, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:44837', name: 68, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:44837', name: 68, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:36601', name: 85, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:44837', name: 68, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:37559', name: 94, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:37559', name: 94, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:37559', name: 94, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:32771', name: 36, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:32771', name: 36, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:37183', name: 47, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:37183', name: 47, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:42651', name: 83, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:42651', name: 83, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:36199', name: 95, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:36199', name: 95, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:33587', name: 74, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:33587', name: 74, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:41371', name: 80, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:41371', name: 80, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:40381', name: 89, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:40381', name: 89, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:32771', name: 36, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:37183', name: 47, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:42651', name: 83, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:36199', name: 95, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:33587', name: 74, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:41371', name: 80, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:40381', name: 89, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:34439', name: 86, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:34439', name: 86, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:38431', name: 87, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:38431', name: 87, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:33659', name: 39, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:33659', name: 39, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:41035', name: 34, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:41035', name: 34, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:33775', name: 81, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:33775', name: 81, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:33525', name: 167, memory: 0, processing: 2>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:33525', name: 167, memory: 0, processing: 2>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:39863', name: 92, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:39863', name: 92, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:40019', name: 171, memory: 0, processing: 2>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:40019', name: 171, memory: 0, processing: 2>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:33391', name: 82, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:33391', name: 82, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:43629', name: 142, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:43629', name: 142, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:44385', name: 70, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:44385', name: 70, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:33967', name: 42, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:33967', name: 42, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:34807', name: 77, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:34807', name: 77, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:34107', name: 128, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:34107', name: 128, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:38677', name: 138, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:38677', name: 138, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:40193', name: 18, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:40193', name: 18, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:41647', name: 56, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:41647', name: 56, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:39707', name: 27, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:39707', name: 27, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:36797', name: 53, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:36797', name: 53, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:34439', name: 86, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:38431', name: 87, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:33659', name: 39, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:41035', name: 34, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:33775', name: 81, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:33525', name: 167, memory: 0, processing: 2>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:39863', name: 92, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:40019', name: 171, memory: 0, processing: 2>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:33391', name: 82, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:43629', name: 142, memory: 0, processing: 0>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:44385', name: 70, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:33967', name: 42, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:34807', name: 77, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:34107', name: 128, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:38677', name: 138, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:40193', name: 18, memory: 0, processing: 0>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:41647', name: 56, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:39707', name: 27, memory: 0, processing: 0>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:36797', name: 53, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:38729', name: 141, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:38729', name: 141, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:42165', name: 62, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:42165', name: 62, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:42043', name: 79, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:42043', name: 79, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:34349', name: 35, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:34349', name: 35, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:41379', name: 21, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:41379', name: 21, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:41697', name: 161, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:41697', name: 161, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:35809', name: 175, memory: 0, processing: 2>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:35809', name: 175, memory: 0, processing: 2>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:46565', name: 165, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:46565', name: 165, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:39275', name: 67, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:39275', name: 67, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:44479', name: 55, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:44479', name: 55, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:36397', name: 63, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:36397', name: 63, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:36983', name: 170, memory: 0, processing: 2>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:36983', name: 170, memory: 0, processing: 2>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:42899', name: 22, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:42899', name: 22, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:40137', name: 137, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:40137', name: 137, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:33507', name: 54, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:33507', name: 54, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:34761', name: 38, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:34761', name: 38, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:46267', name: 57, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:46267', name: 57, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:45387', name: 75, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:45387', name: 75, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:33341', name: 173, memory: 0, processing: 2>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:33341', name: 173, memory: 0, processing: 2>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:40731', name: 166, memory: 0, processing: 2>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:40731', name: 166, memory: 0, processing: 2>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:40073', name: 60, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:40073', name: 60, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:41257', name: 24, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:41257', name: 24, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:37705', name: 50, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:37705', name: 50, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:42191', name: 66, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:42191', name: 66, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:39435', name: 48, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:39435', name: 48, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:41287', name: 129, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:41287', name: 129, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:43473', name: 37, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:43473', name: 37, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:34769', name: 32, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:34769', name: 32, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:44389', name: 23, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:44389', name: 23, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:42841', name: 65, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:42841', name: 65, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:45477', name: 64, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:45477', name: 64, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:37239', name: 17, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:37239', name: 17, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:37821', name: 26, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:37821', name: 26, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:34351', name: 78, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:34351', name: 78, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:41787', name: 168, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:41787', name: 168, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:36715', name: 143, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:36715', name: 143, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:43953', name: 76, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:43953', name: 76, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:38959', name: 130, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:38959', name: 130, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:43777', name: 136, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:43777', name: 136, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:38245', name: 33, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:38245', name: 33, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:39585', name: 45, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:39585', name: 45, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:37117', name: 29, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:37117', name: 29, memory: 0, processing: 0>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:38729', name: 141, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:42165', name: 62, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:42043', name: 79, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:34349', name: 35, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:41379', name: 21, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:41697', name: 161, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:35809', name: 175, memory: 0, processing: 2>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:46565', name: 165, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:39275', name: 67, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:44479', name: 55, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:36397', name: 63, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:36983', name: 170, memory: 0, processing: 2>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:42899', name: 22, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:40137', name: 137, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:33507', name: 54, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:34761', name: 38, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:46267', name: 57, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:45387', name: 75, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:33341', name: 173, memory: 0, processing: 2>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:40731', name: 166, memory: 0, processing: 2>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:40073', name: 60, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:41257', name: 24, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:37705', name: 50, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:42191', name: 66, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:39435', name: 48, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:41287', name: 129, memory: 0, processing: 0>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:43473', name: 37, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:34769', name: 32, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:44389', name: 23, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:42841', name: 65, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:45477', name: 64, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:37239', name: 17, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:37821', name: 26, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:34351', name: 78, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:41787', name: 168, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:36715', name: 143, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:43953', name: 76, memory: 0, processing: 0>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:38959', name: 130, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:43777', name: 136, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:38245', name: 33, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:39585', name: 45, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:37117', name: 29, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:33417', name: 61, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:33417', name: 61, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:45421', name: 69, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:45421', name: 69, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:46345', name: 16, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:46345', name: 16, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:33417', name: 61, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:45421', name: 69, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:46345', name: 16, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:34287', name: 52, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:34287', name: 52, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:41051', name: 41, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:41051', name: 41, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:46525', name: 134, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:46525', name: 134, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:38113', name: 51, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:38113', name: 51, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:34287', name: 52, memory: 0, processing: 0>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:41051', name: 41, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:46525', name: 134, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:38113', name: 51, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:36645', name: 58, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:36645', name: 58, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:40665', name: 59, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:40665', name: 59, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:36857', name: 162, memory: 0, processing: 2>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:36857', name: 162, memory: 0, processing: 2>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:36645', name: 58, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:40665', name: 59, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:36857', name: 162, memory: 0, processing: 2>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:33115', name: 20, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:33115', name: 20, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:40967', name: 164, memory: 0, processing: 2>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:40967', name: 164, memory: 0, processing: 2>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:44923', name: 132, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:44923', name: 132, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:42547', name: 93, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:42547', name: 93, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:34457', name: 46, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:34457', name: 46, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.251:44393', name: 49, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:44393', name: 49, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:43859', name: 44, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:43859', name: 44, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:39633', name: 19, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:39633', name: 19, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:33115', name: 20, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:40967', name: 164, memory: 0, processing: 2>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:44923', name: 132, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:42547', name: 93, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:34457', name: 46, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.251:44393', name: 49, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:43859', name: 44, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:39633', name: 19, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:40581', name: 25, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:40581', name: 25, memory: 0, processing: 0>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:40581', name: 25, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:42017', name: 84, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:42017', name: 84, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:42017', name: 84, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:35699', name: 135, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:35699', name: 135, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:35699', name: 135, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:40607', name: 31, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:40607', name: 31, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:40607', name: 31, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:36273', name: 40, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:36273', name: 40, memory: 0, processing: 0>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:36273', name: 40, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:37521', name: 91, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:37521', name: 91, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:37521', name: 91, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:43851', name: 140, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:43851', name: 140, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:43851', name: 140, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:45741', name: 160, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:45741', name: 160, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:45741', name: 160, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:39323', name: 169, memory: 0, processing: 2>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:39323', name: 169, memory: 0, processing: 2>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:39323', name: 169, memory: 0, processing: 2>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:40431', name: 73, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:40431', name: 73, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:40431', name: 73, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:39269', name: 152, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:39269', name: 152, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:39269', name: 152, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:42015', name: 146, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:42015', name: 146, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:42015', name: 146, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:36605', name: 145, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:36605', name: 145, memory: 0, processing: 0>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:36605', name: 145, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:42879', name: 72, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:42879', name: 72, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:45007', name: 155, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:45007', name: 155, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:39643', name: 151, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:39643', name: 151, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:40195', name: 150, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:40195', name: 150, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:40571', name: 149, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:40571', name: 149, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:42245', name: 148, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:42245', name: 148, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:35899', name: 156, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:35899', name: 156, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:33165', name: 153, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:33165', name: 153, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:40049', name: 147, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:40049', name: 147, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:42879', name: 72, memory: 0, processing: 0>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:45007', name: 155, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:39643', name: 151, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:40195', name: 150, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:40571', name: 149, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:42245', name: 148, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:35899', name: 156, memory: 0, processing: 0>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:33165', name: 153, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:40049', name: 147, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:40601', name: 157, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:40601', name: 157, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:40601', name: 157, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:46263', name: 133, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:46263', name: 133, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:46263', name: 133, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:35215', name: 139, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:35215', name: 139, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:35215', name: 139, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:43247', name: 154, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:43247', name: 154, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:43247', name: 154, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.3:35913', name: 90, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:35913', name: 90, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.3:35913', name: 90, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.49.252:36385', name: 71, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:36385', name: 71, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.49.252:36385', name: 71, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:33495', name: 144, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:33495', name: 144, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:33495', name: 144, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.192:35407', name: 43, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:35407', name: 43, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.192:35407', name: 43, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:35419', name: 30, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:35419', name: 30, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:35419', name: 30, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.48:32777', name: 131, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:32777', name: 131, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.48:32777', name: 131, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.8.191:46779', name: 28, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:46779', name: 28, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.8.191:46779', name: 28, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.79:37611', name: 172, memory: 0, processing: 2>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:37611', name: 172, memory: 0, processing: 2>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.79:37611', name: 172, memory: 0, processing: 2>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:44035', name: 125, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:44035', name: 125, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:44035', name: 125, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:42235', name: 121, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:42235', name: 121, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:34001', name: 114, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:34001', name: 114, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:33859', name: 115, memory: 0, processing: 0>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:33859', name: 115, memory: 0, processing: 0>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:42235', name: 121, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:34001', name: 114, memory: 0, processing: 0>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:33859', name: 115, memory: 0, processing: 0>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:41103', name: 123, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:41103', name: 123, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:37573', name: 122, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:37573', name: 122, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:33037', name: 112, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:33037', name: 112, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:37365', name: 113, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:37365', name: 113, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:45735', name: 118, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:45735', name: 118, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:39241', name: 127, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:39241', name: 127, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:36655', name: 124, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:36655', name: 124, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:44307', name: 117, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:44307', name: 117, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:41103', name: 123, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:37573', name: 122, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:33037', name: 112, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:37365', name: 113, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:45735', name: 118, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:39241', name: 127, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:36655', name: 124, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:44307', name: 117, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:35515', name: 116, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:35515', name: 116, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:39003', name: 119, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:39003', name: 119, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:35515', name: 116, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:39003', name: 119, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:45689', name: 120, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:45689', name: 120, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:45689', name: 120, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:41187', name: 109, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:41187', name: 109, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:39335', name: 103, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:39335', name: 103, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:41187', name: 109, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:39335', name: 103, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:34035', name: 101, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:34035', name: 101, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:34035', name: 101, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:35047', name: 108, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:35047', name: 108, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:45811', name: 107, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:45811', name: 107, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:35721', name: 102, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:35721', name: 102, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:33231', name: 104, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:33231', name: 104, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:35047', name: 108, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:45811', name: 107, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:35721', name: 102, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:33231', name: 104, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:35959', name: 110, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:35959', name: 110, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:35707', name: 97, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:35707', name: 97, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:44497', name: 99, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:44497', name: 99, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:42285', name: 111, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:42285', name: 111, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:38853', name: 98, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:38853', name: 98, memory: 0, processing: 1>
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:42213', name: 96, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:42213', name: 96, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:35959', name: 110, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:35707', name: 97, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:44497', name: 99, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:42285', name: 111, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:38853', name: 98, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:42213', name: 96, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:39691', name: 158, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:39691', name: 158, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:39691', name: 158, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:39383', name: 106, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:39383', name: 106, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:39383', name: 106, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.78:43245', name: 159, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:43245', name: 159, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.78:43245', name: 159, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.4:37815', name: 105, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:37815', name: 105, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.4:37815', name: 105, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.utils - ERROR - Worker already exists <Worker 'tcp://10.128.50.47:36447', name: 126, memory: 0, processing: 1>
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:36447', name: 126, memory: 0, processing: 1>
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3711, in add_worker
    raise ValueError("Worker already exists %s" % ws)
ValueError: Worker already exists <Worker 'tcp://10.128.50.47:36447', name: 126, memory: 0, processing: 1>
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.utils - ERROR - 'CalculateMap-7f72b3eb-ef20-452e-9fc4-aa2e34dfd845'
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/utils.py", line 666, in log_errors
    yield
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-7f72b3eb-ef20-452e-9fc4-aa2e34dfd845'
distributed.core - ERROR - Exception while handling op register-worker
Traceback (most recent call last):
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/core.py", line 500, in handle_comm
    result = await result
  File "/opt/miniconda3/lib/python3.8/site-packages/distributed/scheduler.py", line 3794, in add_worker
    typename=types[key],
KeyError: 'CalculateMap-7f72b3eb-ef20-452e-9fc4-aa2e34dfd845'
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - INFO - Waiting to connect to:   tcp://10.128.5.206:44019
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.79:33179', name: 174, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.79:33179
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://10.128.50.4:45151', name: 100, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.128.50.4:45151
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.128.5.206:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.utils_perf - INFO - full garbage collection released 46.82 MB from 374 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
slurmstepd: error: *** STEP 53576123.0 ON nid01475 CANCELLED AT 2022-01-24T02:25:15 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 53576123 ON nid01475 CANCELLED AT 2022-01-24T02:25:15 DUE TO TIME LIMIT ***
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.79:33179', name: 174, memory: 0, processing: 0>
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
distributed.core - INFO - Removing comms to tcp://10.128.50.79:33179
distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.128.50.4:45151', name: 100, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://10.128.50.4:45151
srun: got SIGCONT
srun: forcing job termination
